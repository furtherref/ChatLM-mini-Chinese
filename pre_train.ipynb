{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-22T05:13:39.805303Z",
     "start_time": "2024-01-22T05:13:37.593863Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import PreTrainedTokenizerFast, Seq2SeqTrainer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments\n",
    "from transformers.generation.configuration_utils import GenerationConfig\n",
    "\n",
    "from model.chat_model import TextToTextModel\n",
    "from model.dataset import MyDataset\n",
    "from config import TrainConfig, T5ModelConfig\n",
    "from utils.functions import json_to_dataclass, get_T5_config"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T05:13:39.810346Z",
     "start_time": "2024-01-22T05:13:39.806352Z"
    }
   },
   "id": "952b42b1e4bdf240",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class My_DataCollatorForSeq2Seq(DataCollatorForSeq2Seq):\n",
    "    def __call__(self, features, return_tensors=None):\n",
    "        '''\n",
    "        将文本编码为id，MyDataset的`__getitem__`方法返回的是: (str, str)\n",
    "        features:list[tuple[str, str]]\n",
    "        '''\n",
    "        prompt = [item[0] for item in features]\n",
    "        resopnse = [item[1] for item in features]\n",
    "\n",
    "        tokenizer = self.tokenizer\n",
    "        prompt_encoded = tokenizer(prompt, padding=False, return_token_type_ids=False, return_attention_mask=False)[\n",
    "            'input_ids']\n",
    "        resopnse_encoded = tokenizer(resopnse, padding=False, return_token_type_ids=False, return_attention_mask=False)[\n",
    "            'input_ids']\n",
    "\n",
    "        batch_size = len(features)\n",
    "        data = []\n",
    "        for i in range(batch_size):\n",
    "            data.append(\n",
    "                {\n",
    "                    'input_ids': prompt_encoded[i],\n",
    "                    'labels': resopnse_encoded[i]\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return super().__call__(data, return_tensors)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T05:13:39.813045Z",
     "start_time": "2024-01-22T05:13:39.810684Z"
    }
   },
   "id": "b0a3347b61c22c2e",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def pre_train(config: TrainConfig) -> None:\n",
    "    # step 1. 加载tokenizer\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(config.tokenizer_dir)\n",
    "    tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True\n",
    "\n",
    "    # step 2. 加载模型配置文件\n",
    "    t5_config = get_T5_config(T5ModelConfig(), vocab_size=len(tokenizer), decoder_start_token_id=tokenizer.pad_token_id,\n",
    "                              eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # step 3. 初始化模型\n",
    "    model = TextToTextModel(t5_config)\n",
    "\n",
    "    # Step 4: Load my dataset\n",
    "    dataset = MyDataset(\n",
    "        parquet_file=config.train_file,\n",
    "        tokenizer_dir=config.tokenizer_dir,\n",
    "        buffer_size=40960,\n",
    "    )\n",
    "\n",
    "    # Step 5: Define the training arguments\n",
    "\n",
    "    # T5属于sequence to sequence模型，故要使用Seq2SeqTrainingArguments、DataCollatorForSeq2Seq、Seq2SeqTrainer\n",
    "    # huggingface官网的sft工具适用于language model/LM模型\n",
    "\n",
    "    generation_config = GenerationConfig()\n",
    "    generation_config.remove_invalid_values = True\n",
    "    generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "    generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "    generation_config.decoder_start_token_id = tokenizer.pad_token_id\n",
    "    generation_config.max_new_tokens = 320\n",
    "    generation_config.num_beams = 1  # greedy search\n",
    "    generation_config.do_sample = False  # greedy search\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=config.output_dir,\n",
    "        per_device_train_batch_size=config.batch_size_per_gpu,\n",
    "        auto_find_batch_size=True,  # 防止OOM\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        learning_rate=config.learn_rate,\n",
    "        logging_steps=config.logging_steps,\n",
    "        num_train_epochs=config.epochs,\n",
    "        optim=\"adafactor\",\n",
    "        report_to='tensorboard',\n",
    "        log_level='info',\n",
    "        save_steps=config.save_steps,\n",
    "        save_total_limit=3,\n",
    "        fp16=True if config.mixed_precision == 'fp16' else False,\n",
    "        bf16=True if config.mixed_precision == 'bf16' else False,\n",
    "        logging_first_step=True,\n",
    "        warmup_steps=config.warmup_steps,\n",
    "        seed=config.seed,\n",
    "        generation_config=generation_config,\n",
    "        # use_cpu=True\n",
    "    )\n",
    "    print(training_args.device)\n",
    "\n",
    "    # step 6: init my collator,\n",
    "    collator = My_DataCollatorForSeq2Seq(tokenizer, max_length=config.max_seq_len)\n",
    "\n",
    "    # Step 7: Define the Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        eval_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "\n",
    "    # step 8: train\n",
    "    trainer.train(\n",
    "        # resume_from_checkpoint=True\n",
    "    )\n",
    "\n",
    "    # step 9: save log\n",
    "    loss_log = pd.DataFrame(trainer.state.log_history)\n",
    "    loss_log.to_csv(f\"./logs/pre_train_log_{time.strftime('%Y%m%d-%H%M')}.csv\")\n",
    "\n",
    "    # Step 10: Save the model\n",
    "    trainer.save_model(config.output_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T05:13:39.816395Z",
     "start_time": "2024-01-22T05:13:39.812606Z"
    }
   },
   "id": "8bda0aea5d5c82ed",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "BF16 Mixed precision training with AMP (`--bf16`) and BF16 half precision evaluation (`--bf16_full_eval`) can only be used on CUDA, XPU (with IPEX), NPU or CPU/TPU/NeuronCore devices.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m config \u001B[38;5;241m=\u001B[39m TrainConfig()\n\u001B[0;32m----> 2\u001B[0m \u001B[43mpre_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[5], line 34\u001B[0m, in \u001B[0;36mpre_train\u001B[0;34m(config)\u001B[0m\n\u001B[1;32m     31\u001B[0m generation_config\u001B[38;5;241m.\u001B[39mnum_beams \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m  \u001B[38;5;66;03m# greedy search\u001B[39;00m\n\u001B[1;32m     32\u001B[0m generation_config\u001B[38;5;241m.\u001B[39mdo_sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m  \u001B[38;5;66;03m# greedy search\u001B[39;00m\n\u001B[0;32m---> 34\u001B[0m training_args \u001B[38;5;241m=\u001B[39m \u001B[43mSeq2SeqTrainingArguments\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[43m    \u001B[49m\u001B[43mper_device_train_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_size_per_gpu\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[43m    \u001B[49m\u001B[43mauto_find_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# 防止OOM\u001B[39;49;00m\n\u001B[1;32m     38\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgradient_accumulation_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgradient_accumulation_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlogging_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogging_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_train_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43madafactor\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreport_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtensorboard\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     44\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlog_level\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minfo\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_total_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     47\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfp16\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmixed_precision\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfp16\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbf16\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmixed_precision\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbf16\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlogging_first_step\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwarmup_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwarmup_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# use_cpu=True\u001B[39;49;00m\n\u001B[1;32m     54\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28mprint\u001B[39m(training_args\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m# step 6: init my collator,\u001B[39;00m\n",
      "File \u001B[0;32m<string>:126\u001B[0m, in \u001B[0;36m__init__\u001B[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, sortish_sampler, predict_with_generate, generation_max_length, generation_num_beams, generation_config)\u001B[0m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/training_args.py:1515\u001B[0m, in \u001B[0;36mTrainingArguments.__post_init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1499\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1501\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX).\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1502\u001B[0m     )\n\u001B[1;32m   1504\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1505\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1506\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m is_torch_available()\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1513\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbf16 \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbf16_full_eval)\n\u001B[1;32m   1514\u001B[0m ):\n\u001B[0;32m-> 1515\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1516\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBF16 Mixed precision training with AMP (`--bf16`) and BF16 half precision evaluation\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1517\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m (`--bf16_full_eval`) can only be used on CUDA, XPU (with IPEX), NPU or CPU/TPU/NeuronCore devices.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1518\u001B[0m     )\n\u001B[1;32m   1520\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtorchdynamo \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1521\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   1522\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`torchdynamo` is deprecated and will be removed in version 5 of 🤗 Transformers. Use\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1523\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m `torch_compile_backend` instead\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1524\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[1;32m   1525\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: BF16 Mixed precision training with AMP (`--bf16`) and BF16 half precision evaluation (`--bf16_full_eval`) can only be used on CUDA, XPU (with IPEX), NPU or CPU/TPU/NeuronCore devices."
     ]
    }
   ],
   "source": [
    "config = TrainConfig()\n",
    "pre_train(config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T05:13:42.151746Z",
     "start_time": "2024-01-22T05:13:39.817662Z"
    }
   },
   "id": "56384972fb0a22ea",
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
